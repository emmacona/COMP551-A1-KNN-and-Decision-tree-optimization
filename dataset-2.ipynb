{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Dataset #2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "# 1. Load the datasets into Pandas dataframes\n",
    "names2=['A1-Time','A2','A3','A4','A5','A6','A7','A8','A9','LABEL']\n",
    "#Full training set will be split into training and validation\n",
    "dataset2_trn_full = pd.read_csv('shuttle_trn.csv', index_col=False, names=names2, sep=' ', engine='python') \n",
    "dataset2_test_full = pd.read_csv('shuttle_tst.csv', index_col=False, names=names2, sep=' ', engine='python')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data visulization & Cleaning\n",
    "### Note: Data is complete, no missing entries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizing data to have a better idea of the data\n",
    "# Plotting frequency of different attribute values since there are only 9 (+ 1 for assigned labels)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Visualize all numerical categories\n",
    "dataset2_trn_full.hist(figsize=(20, 14))\n",
    "\n",
    "'''\n",
    "#removing outliers\n",
    "subset=dataset2_trn.loc[:,'A2':'A9']\n",
    "print(subset)\n",
    "z_scores = stats.zscore(subset)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "print(filtered_entries)\n",
    "dataset2_trn = dataset2_trn[filtered_entries]'''\n",
    "\n",
    "print(dataset2_trn_full.describe())\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalizing data since scales of some attributes are drastically different\n",
    "from sklearn import preprocessing\n",
    "names2_x=['A1-Time','A2','A3','A4','A5','A6','A7','A8','A9']\n",
    "x_d2=dataset2_trn_full.loc[:,'A1-Time':'A9']\n",
    "y_d2=dataset2_trn_full.loc[:,'LABEL']\n",
    "\n",
    "'''min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x_train)\n",
    "normalized_dataset_2 = pd.DataFrame(x_scaled, columns=names2)\n",
    "\n",
    "normalized_dataset_2.hist(figsize=(20, 14))'''\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# fit the scaler \n",
    "scaler.fit(x_d2)\n",
    "\n",
    "# transform the data\n",
    "x_train_normal = scaler.transform(x_d2)\n",
    "\n",
    "normalized_xd2 = pd.DataFrame(x_train_normal, columns=names2_x)\n",
    "\n",
    "normalized_xd2.hist(figsize=(20, 14))\n",
    "#y_d2.hist(figsize=(20,14))\n",
    "print(normalized_xd2.describe())\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-hot coding\n",
    "### No need for one-hot coding since all attributes and labels are numerical values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments Dataset #2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split training data into training and validation, start with 9:1 ratio (90% traning, 10% validation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def splitTraining(p_val,num_instances):\n",
    "    n_val=num_instances//p_val\n",
    "    np.random.seed(5)\n",
    "    inds = np.random.permutation(num_instances)\n",
    "    x_val2,y_val2=normalized_xd2.loc[inds[:n_val],:],y_d2.loc[inds[:n_val]]\n",
    "    x_train2,y_train2=normalized_xd2.loc[inds[n_val:],:],y_d2.loc[inds[n_val:]]\n",
    "    \n",
    "    return n_val, x_val2,y_val2, x_train2, y_train2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross validation method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cross_validate(total_nval, n_folds=5):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = total_nval // n_folds\n",
    "    np.random.seed(0)\n",
    "    inds = np.random.permutation(total_nval)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexess\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, total_nval))\n",
    "        #The yield statement suspends functionâ€™s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up training/validation data sets and variables needed for hyperparameter analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(trn_instances, num_features) = dataset2_trn_full.shape\n",
    "num_folds = 5\n",
    "\n",
    "#split the data\n",
    "(n_val_25, x_val2_25,y_val2_25, x_train2_75, y_train2_75)=splitTraining(25,int(0.3*trn_instances))\n",
    "(n_val_50, x_val2_50,y_val2_50, x_train2_50, y_train2_50)=splitTraining(50,int(0.3*trn_instances))\n",
    "(n_val_75, x_val2_75,y_val2_75, x_train2_25, y_train2_25)=splitTraining(75,int(0.3*trn_instances))\n",
    "\n",
    "#converting training and validation dataframe sets to numpy arrays\n",
    "x_val2_25=x_val2_25.to_numpy()\n",
    "y_val2_25=y_val2_25.to_numpy()\n",
    "x_train2_75=x_train2_75.to_numpy()\n",
    "y_train2_75=y_train2_75.to_numpy()\n",
    "\n",
    "\n",
    "x_val2_50=x_val2_50.to_numpy()\n",
    "y_val2_50=y_val2_50.to_numpy()\n",
    "x_train2_50=x_train2_50.to_numpy()\n",
    "y_train2_50=y_train2_50.to_numpy()\n",
    "\n",
    "x_val2_75=x_val2_75.to_numpy()\n",
    "y_val2_75=y_val2_75.to_numpy()\n",
    "x_train2_25=x_train2_25.to_numpy()\n",
    "y_train2_25=y_train2_25.to_numpy()\n",
    "\n",
    "#define a function for the accuracy\n",
    "accuracy= lambda y, yh: np.mean(y==yh)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating best number of K neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "\n",
    "def find_bestK(n_val,x_train,y_train,x_val,y_val,title):\n",
    "    K_list=range(1,100)\n",
    "    acc_train,acc_valid = np.zeros(len(K_list)), np.zeros((len(K_list), num_folds))\n",
    "    #Evaluating best number of K neighbors\n",
    "    for i, K in enumerate(K_list):\n",
    "        #Find the validation errors for num_folds splits for a given K\n",
    "        model = neighbors.KNeighborsRegressor(n_neighbors=K)\n",
    "        model = model.fit(x_train, y_train)\n",
    "        acc_train[i]= accuracy(y_train, model.predict(x_train))\n",
    "        for f, (tr, val) in enumerate(cross_validate(n_val, num_folds)):\n",
    "            model = neighbors.KNeighborsRegressor(n_neighbors=K)\n",
    "            model = model.fit(x_val[tr,:], y_val[tr])\n",
    "            acc_valid[i, f] = accuracy(y_val[val], model.predict(x_val[val])) \n",
    "    \n",
    "    plt.plot(K_list, acc_train,  label='train')\n",
    "    plt.errorbar(K_list, np.mean(acc_valid, axis=1), np.std(acc_valid, axis=1), label='validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('K (number of neighbours)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    plt.title(title)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating neighbor weights"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "\n",
    "#Find best K with weighted neighbors\n",
    "def find_bestK_weighted(n_val,x_train,y_train,x_val,y_val,title):\n",
    "    K_list=range(1,100)\n",
    "    acc_train,acc_valid = np.zeros(len(K_list)), np.zeros((len(K_list), num_folds))\n",
    "    #Evaluating best number of K neighbors\n",
    "    for i, K in enumerate(K_list):\n",
    "        #Find the validation errors for num_folds splits for a given K\n",
    "        model = neighbors.KNeighborsRegressor(n_neighbors=K,weights='distance')\n",
    "        model = model.fit(x_train, y_train)\n",
    "        acc_train[i]= accuracy(y_train, model.predict(x_train))\n",
    "        for f, (tr, val) in enumerate(cross_validate(n_val, num_folds)):\n",
    "            model = neighbors.KNeighborsRegressor(n_neighbors=K,weights='distance')\n",
    "            model = model.fit(x_val[tr,:], y_val[tr])\n",
    "            acc_valid[i, f] = accuracy(y_val[val], model.predict(x_val[val])) \n",
    "    \n",
    "    plt.plot(K_list, acc_train,  label='train')\n",
    "    plt.errorbar(K_list, np.mean(acc_valid, axis=1), np.std(acc_valid, axis=1), label='validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('K (number of neighbours)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    plt.title(title)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating best distance to use (Euclidean or Manhattan)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verifying custom cross-validation results against cross-validation package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#add sklearn cross val method for verification purposes\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating best split criterion without max depth limitation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#Find best split criteria\n",
    "\n",
    "def find_best_crit(n_val,x_train,y_train,x_val,y_val,title):\n",
    "        criteria_list=[\"gini\", \"entropy\"] \n",
    "        acc_train,acc_valid = np.zeros(len(criteria_list)), np.zeros((len(criteria_list), num_folds))\n",
    "\n",
    "        model = tree.DecisionTreeClassifier()\n",
    "        model = model.fit(x_train, y_train)\n",
    "        acc_train[0]= accuracy(y_train, model.predict(x_train))\n",
    "\n",
    "\n",
    "        for f, (tr, val) in enumerate(cross_validate(n_val, num_folds)):\n",
    "                model = tree.DecisionTreeClassifier()\n",
    "                model = model.fit(x_val[tr,:], y_val[tr])\n",
    "                acc_valid[0,f] = accuracy(y_val[val], model.predict(x_val[val])) \n",
    "\n",
    "\n",
    "        model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        model = model.fit(x_train, y_train)\n",
    "        acc_train[1]= accuracy(y_train, model.predict(x_train))\n",
    "\n",
    "        for f, (tr, val) in enumerate(cross_validate(n_val, num_folds)):\n",
    "                model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "                model = model.fit(x_val[tr,:], y_val[tr])\n",
    "                acc_valid[1,f] = accuracy(y_val[val], model.predict(x_val[val])) \n",
    "\n",
    "\n",
    "\n",
    "        x=[1,2]\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(x,acc_train, label='training')\n",
    "        plt.xlabel('Gini Index=1                Entropy=2')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training')\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.scatter(x,np.mean(acc_valid, axis=1),label='validation')\n",
    "        #plt.errorbar(criteria_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='Validation')\n",
    "        plt.title('Validation')\n",
    "        plt.xlabel('Gini Index=1                Entropy=2')\n",
    "        plt.ylabel('Accuracy')\n",
    "\n",
    "        plt.tight_layout(pad=5)\n",
    "        plt.show()\n",
    "        plt.title(title)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating best max depth limitation with Gini index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import tree\n",
    "\n",
    "\n",
    "#Find best max depth using Gini index\n",
    "\n",
    "def find_maxD_gini(n_val,x_train,y_train,x_val,y_val,title):\n",
    "    max_depth=range(4,8)\n",
    "\n",
    "    acc_train,acc_valid = np.zeros(len(max_depth)), np.zeros((len(max_depth), num_folds))\n",
    "\n",
    "\n",
    "    for i, d in enumerate(max_depth):\n",
    "        #Find the validation errors for num_folds splits for a given max_depth\n",
    "        model = tree.DecisionTreeClassifier(criterion='gini',max_depth=d)\n",
    "        model = model.fit(x_train, y_train)\n",
    "        acc_train[i]= accuracy(y_train, model.predict(x_train))\n",
    "        for f, (tr, val) in enumerate(cross_validate(n_val, num_folds)):\n",
    "            model = tree.DecisionTreeClassifier(criterion='gini',max_depth=d)\n",
    "            model = model.fit(x_val[tr,:], y_val[tr])\n",
    "            acc_valid[i, f] = accuracy(y_val[val], model.predict(x_val[val])) \n",
    "    \n",
    "    plt.plot(max_depth, acc_train,  label='Train')\n",
    "    plt.errorbar(max_depth, np.mean(acc_valid, axis=1), np.std(acc_valid, axis=1), label='Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Max depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating best max depth limitation with Entropy "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#Find best max depth using Gini index\n",
    "\n",
    "def find_minsamp_entropy(n_val,x_train,y_train,x_val,y_val,title):\n",
    "    max_depth=range(4,8)\n",
    "\n",
    "    acc_train,acc_valid = np.zeros(len(max_depth)), np.zeros((len(max_depth), num_folds))\n",
    "\n",
    "    #Evaluating value for max depth\n",
    "    for i, d in enumerate(max_depth):\n",
    "        #Find the validation errors for num_folds splits for a max_depth\n",
    "        model = tree.DecisionTreeClassifier(criterion='entropy',max_depth=d)\n",
    "        model = model.fit(x_train, y_train)\n",
    "        acc_train[i]= accuracy(y_train, model.predict(x_train))\n",
    "        for f, (tr, val) in enumerate(cross_validate(n_val, num_folds)):\n",
    "            model = tree.DecisionTreeClassifier(criterion='entropy',max_depth=d)\n",
    "            model = model.fit(x_val[tr,:], y_val[tr])\n",
    "            acc_valid[i, f] = accuracy(y_val[val], model.predict(x_val[val])) \n",
    "    \n",
    "    plt.plot(max_depth, acc_train,  label='Train')\n",
    "    plt.errorbar(max_depth, np.mean(acc_valid, axis=1), np.std(acc_valid, axis=1), label='Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Max depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conclusion: Entropy gives best results, max depth does not affect MSE (if we were too choose max depth, then 4 (?) should be a good number)\n",
    "Note: 0 MSE on training error (?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the best minimum number of samples using Entropy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Find best min number of samples using Entropy\n",
    "\n",
    "def find_minsamp_entrop(n_val,x_train,y_train,x_val,y_val,title):\n",
    "    min_sample=range(3,8)\n",
    "\n",
    "    acc_train,acc_valid = np.zeros(len(min_sample)), np.zeros((len(min_sample), num_folds))\n",
    "\n",
    "    #Evaluating value for max depth\n",
    "    for i, d in enumerate(min_sample):\n",
    "        #Find the validation errors for num_folds splits for a max_depth\n",
    "        model = tree.DecisionTreeClassifier(criterion='entropy',max_depth=d)\n",
    "        model = model.fit(x_train, y_train)\n",
    "        acc_train[i]= accuracy(y_train, model.predict(x_train))\n",
    "        for f, (tr, val) in enumerate(cross_validate(n_val, num_folds)):\n",
    "            model = tree.DecisionTreeClassifier(criterion='entropy',max_depth=d)\n",
    "            model = model.fit(x_val[tr,:], y_val[tr])\n",
    "            acc_valid[i, f] = accuracy(y_val[val], model.predict(x_val[val])) \n",
    "    \n",
    "    plt.plot(min_sample, acc_train,  label='Train')\n",
    "    plt.errorbar(min_sample, np.mean(acc_valid, axis=1), np.std(acc_valid, axis=1), label='Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Max depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calling training models with hyperparameter evaluations on growing subsets of training/validation data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments on test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import neighbors\n",
    "\n",
    "x_d2_test=dataset2_test_full.loc[:,'A1-Time':'A9']\n",
    "y_d2_test=dataset2_test_full.loc[:,'LABEL']\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# fit the scaler \n",
    "scaler.fit(x_d2_test)\n",
    "\n",
    "# transform the data\n",
    "x_test_normal = scaler.transform(x_d2_test)\n",
    "\n",
    "normalized_xd2_test = pd.DataFrame(x_test_normal, columns=names2_x)\n",
    "\n",
    "x_test2=normalized_xd2_test.to_numpy()\n",
    "y_test2=y_d2_test.to_numpy()\n",
    "\n",
    "'''normalized_xd2.hist(figsize=(20, 14))\n",
    "#y_d2.hist(figsize=(20,14))\n",
    "print(normalized_xd2.describe())'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN (K = 2, weighted neighbors (or not? performs similarly with and without), Euclidian distance)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_model_KNN = neighbors.KNeighborsClassifier(n_neighbors=2,weights='distance')\n",
    "best_model_KNN.fit(x_test2, y_test2)\n",
    "test_pred = best_model_KNN.predict(x_test2)\n",
    "\n",
    "acc = np.mean(test_pred == y_test2)\n",
    "print(confusion_matrix(y_test2, test_pred), f'{acc:.3f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree (Splite criteria = Entropy, max depth = 5)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_model_dt = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "best_model_dt.fit(x_test2, y_test2)\n",
    "test_pred_dt = best_model_dt.predict(x_test2)\n",
    "\n",
    "acc = np.mean(test_pred_dt == y_test2)\n",
    "print(confusion_matrix(y_test2, test_pred_dt), f'{acc:.3f}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}